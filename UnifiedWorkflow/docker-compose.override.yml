services:
  api:
    ports:
      - "8000:8000"  # Expose API port for testing
    environment:
      - WORKERS=4
      - WORKER_MEMORY_LIMIT=512MB
      - DB_POOL_SIZE=10
      - DB_POOL_OVERFLOW=20
      - PYTHONPATH=/project/app
    entrypoint: []
    command:
      - uvicorn
      - api.main:app
      - --host
      - "0.0.0.0"
      - --port
      - "8000"
    # Enhanced health check configuration to prevent service disruption
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:8000/health --max-time 10 || exit 1
      interval: 45s
      timeout: 15s
      retries: 4
      start_period: 45s
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
  
  webui:
    ports:
      - "3001:3001"  # Expose WebUI port for testing (matching internal PORT env var)
    # Enhanced health check configuration to prevent restart loops
    healthcheck:
      test:
        - CMD-SHELL
        - wget --quiet --tries=2 --spider --timeout=10 http://0.0.0.0:3001/ || exit 1
      interval: 45s
      timeout: 15s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
  
  # Memory optimized Ollama configuration
  ollama:
    environment:
      - OLLAMA_MAX_LOADED_MODELS=3  # Increase to utilize more GPU memory
      - OLLAMA_KEEP_ALIVE=15m       # Keep models loaded longer for better utilization
      - OLLAMA_GPU_OVERHEAD=0.05    # Reduce from 0.1
      - OLLAMA_CONTEXT_LENGTH=16384 # Maintain efficient context
      - OLLAMA_BATCH_SIZE=32        # Increase for better GPU utilization
      - OLLAMA_CONCURRENT_REQUESTS=12 # Increase for higher utilization
      - OLLAMA_NUM_PARALLEL=6       # Optimize for 50%+ GPU utilization
      - OLLAMA_FLASH_ATTENTION=true # Enable memory-efficient attention
      - CUDA_VISIBLE_DEVICES=0,1,2  # Utilize all available GPUs
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  
  # Memory optimized PostgreSQL configuration
  postgres:
    environment:
      - POSTGRES_SHARED_BUFFERS=256MB
      - POSTGRES_EFFECTIVE_CACHE_SIZE=1GB
      - POSTGRES_WORK_MEM=4MB
      - POSTGRES_MAINTENANCE_WORK_MEM=64MB
      - POSTGRES_MAX_CONNECTIONS=100
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
  
  # Memory optimized Redis configuration  
  redis:
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
  
  # Memory optimized services
  hybrid-memory-service:
    environment:
      - VECTOR_CACHE_SIZE=256MB
      - BATCH_PROCESSING_SIZE=100
      - EMBEDDING_CACHE_LIMIT=10000
      - CONCURRENT_OPERATIONS=4
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
  
  reasoning-service:
    environment:
      - REASONING_CACHE_SIZE=128MB
      - MAX_REASONING_DEPTH=5
      - MEMORY_THRESHOLD_CLEANUP=0.8
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
  
  learning-service:
    environment:
      - MODEL_CACHE_SIZE=256MB
      - TRAINING_BATCH_SIZE=32
      - MIXED_PRECISION=true
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
  
  coordination-service:
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
  
  gpu-monitor:
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  
  monitoring-service:
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M