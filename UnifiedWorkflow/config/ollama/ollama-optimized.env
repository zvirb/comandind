# HIGH-PERFORMANCE Ollama Configuration for 3x NVIDIA TITAN X GPUs
# Target: >50% GPU Utilization with Optimal Memory Distribution

# GPU Configuration - Maximize Parallel Processing
OLLAMA_NUM_PARALLEL=8
OLLAMA_CONCURRENT_REQUESTS=16
OLLAMA_FLASH_ATTENTION=true
OLLAMA_GPU_OVERHEAD=0.03

# Multi-GPU Workload Distribution
# Optimally distribute across 3 GPUs with 12GB VRAM each
OLLAMA_TENSOR_SPLIT=4096,4096,4096  # Equal 4GB allocation per GPU
OLLAMA_SCHED_SPREAD=true
CUDA_VISIBLE_DEVICES=0,1,2

# Memory Management - Aggressive GPU Utilization
OLLAMA_MAX_LOADED_MODELS=6  # 2 models per GPU
OLLAMA_KEEP_ALIVE=30m  # Keep models loaded longer
OLLAMA_LOAD_TIMEOUT=5m  # Faster loading

# Performance Optimization - Enhanced Throughput
OLLAMA_CONTEXT_LENGTH=32768  # Maximize context for better utilization
OLLAMA_BATCH_SIZE=64  # Larger batches for GPU efficiency
OLLAMA_KV_CACHE_TYPE=f16
OLLAMA_MAX_QUEUE=1024  # Handle more concurrent requests

# Inference Optimization - GPU Utilization Focus
OLLAMA_EVAL_BATCH_SIZE=32
OLLAMA_ROPE_SCALING_TYPE=linear
OLLAMA_ROPE_FREQ_BASE=10000

# Debug and Monitoring
OLLAMA_DEBUG=INFO
OLLAMA_NOHISTORY=false
OLLAMA_LOG_FORMAT=json

# Host Configuration
OLLAMA_HOST=0.0.0.0:11434
OLLAMA_ORIGINS="*"

# CUDA Optimization - Maximum Performance
CUDA_LAUNCH_BLOCKING=0
CUDA_CACHE_DISABLE=0
CUDA_DEVICE_ORDER=PCI_BUS_ID
CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps
CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log

# Memory Pool Optimization
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,garbage_collection_threshold:0.6

# Performance Monitoring
OLLAMA_METRICS=true
OLLAMA_METRICS_PORT=11435