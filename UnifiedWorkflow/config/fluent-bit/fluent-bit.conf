[SERVICE]
    # Flush
    # =====
    # Set an interval of seconds before to flush records to a destination
    Flush         5

    # Daemon
    # ======
    # Instruct Fluent Bit to run in foreground or background mode.
    Daemon        Off

    # Log_Level
    # =========
    # Set the verbosity level of the service, values can be:
    #
    # - error
    # - warning
    # - info
    # - debug
    # - trace
    #
    # By default 'info' is set, that means it includes 'error' and 'warning'.
    Log_Level     info

    # Parsers_File
    # ============
    # Specify an optional 'Parsers' configuration file
    Parsers_File  parsers.conf
    Plugins_File  plugins.conf

    # HTTP Server
    # ===========
    # Enable/Disable the built-in HTTP Server for metrics
    HTTP_Server   On
    HTTP_Listen   0.0.0.0
    HTTP_Port     2020

    # Storage
    # =======
    # Fluent Bit can use memory or filesystem buffering based mechanisms
    #
    # - https://docs.fluentbit.io/manual/administration/buffering-and-storage
    #
    # storage metrics
    # ---------------
    # publish storage pipeline metrics in '/api/v1/storage'. The metrics are
    # exported only if the 'http_server' option is enabled.
    #
    storage.metrics                on

    # storage.path
    # ------------
    # absolute file system path to store filesystem data buffers (chunks).
    #
    # storage.path              /tmp/storage

    # storage.sync
    # ------------
    # configure the synchronization mode used to store the data into the
    # filesystem. It can take the values normal or full.
    #
    # storage.sync              normal

    # storage.checksum
    # ----------------
    # enable the data integrity check when writing and reading data from the
    # filesystem. The storage layer uses the CRC32 algorithm.
    #
    # storage.checksum          off

    # storage.backlog.mem_limit
    # -------------------------
    # if storage.path is set, Fluent Bit will look for data chunks that were
    # not delivered and are still in the storage layer, these are called
    # backlog data. This option configure a hint of maximum value of memory
    # to use when processing these records.
    #
    # storage.backlog.mem_limit 5M

[INPUT]
    Name              tail
    Path              /app/logs/*.log
    Parser            json
    Tag               ai_workflow_engine.*
    Refresh_Interval  5
    Mem_Buf_Limit     10MB
    Skip_Long_Lines   On
    Buffer_Chunk_Size 32k
    Buffer_Max_Size   64k

[INPUT]
    Name              tail
    Path              /var/lib/docker/containers/*/*.log
    Parser            docker
    Tag               docker.*
    Refresh_Interval  5
    Mem_Buf_Limit     10MB
    Skip_Long_Lines   On
    Buffer_Chunk_Size 32k
    Buffer_Max_Size   64k

# Kubernetes filter removed - not needed for Docker Compose environment
# This prevents the "invalid pattern for given tag" warnings

[FILTER]
    Name          modify
    Match         docker.*
    Add           cluster ai_workflow_engine
    Add           environment production
    Add           source docker_container

[FILTER]
    Name          modify
    Match         ai_workflow_engine.*
    Add           cluster ai_workflow_engine
    Add           environment production

[OUTPUT]
    Name            loki
    Match           *
    Host            loki
    Port            3100
    Labels          job=fluent-bit, cluster=ai_workflow_engine
    Label_keys      $level,$service,$container,$stream
    Remove_keys     timestamp
    Line_format     json
    Auto_kubernetes_labels off