# ==============================================================================
# Reusable YAML Anchors
# ==============================================================================
x-logging: &logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"

x-base-service-properties: &base-service-properties
  restart: unless-stopped
  networks:
    - ai_workflow_engine_net
  logging: *logging

x-gpu-deploy: &gpu-deploy
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]

# ==============================================================================
# Top-Level Keys
# ==============================================================================
networks:
  ai_workflow_engine_net:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  qdrant_data:
  prometheus_data:
  grafana_data:
  alertmanager_data:
  ollama_data:
  certs:
  webui_node_modules: {}
  webui_next_node_modules: {}
  jaeger_data:
  caddy_data:  # Let's Encrypt certificate storage
  caddy_config:  # Caddy configuration persistence
  loki_data:
  elasticsearch_data:
  kibana_data:

secrets:
  admin_email:
    file: ./secrets/admin_email.txt
  admin_password:
    file: ./secrets/admin_password.txt
  google_client_id:
    file: ./secrets/google_client_id.txt
  google_client_secret:
    file: ./secrets/google_client_secret.txt
  JWT_SECRET_KEY:
    file: ./secrets/jwt_secret_key.txt
  CSRF_SECRET_KEY:
    file: ./secrets/csrf_secret_key.txt
  POSTGRES_PASSWORD:
    file: ./secrets/postgres_password.txt
  postgres_user:
    file: ./secrets/postgres_user.txt
  postgres_db:
    file: ./secrets/postgres_db.txt
  QDRANT_API_KEY:
    file: ./secrets/qdrant_api_key.txt
  pgbouncer_users:
    file: ./secrets/pgbouncer_userlist.txt
  REDIS_PASSWORD:
    file: ./secrets/redis_password.txt
  redis_users_acl:
    file: ./secrets/redis_users.acl
  API_KEY:
    file: ./secrets/api_key.txt

# ==============================================================================
# SERVICES
# ==============================================================================
services:
  # New Coordination Service - Agent orchestration and workflow management
  coordination-service:
    build:
      context: ./app/coordination_service
      dockerfile: Dockerfile
    image: ai_workflow_engine/coordination-service
    <<: *base-service-properties
    environment:
      - DATABASE_URL=postgresql://app_user:${POSTGRES_PASSWORD}@postgres:5432/ai_workflow_db
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=1
      - REDIS_USER=lwe-app
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - QDRANT_URL=https://qdrant:6333
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    ports:
      - "8001:8001"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8001/health --max-time 3 || exit 1"]
      interval: 20s
      timeout: 5s
      retries: 3
      start_period: 45s

  # Enhanced Memory Service - Two-phase memory pipeline with LLM processing
  hybrid-memory-service:
    build:
      context: ./app/memory_service
      dockerfile: Dockerfile
    image: ai_workflow_engine/hybrid-memory-service
    <<: *base-service-properties
    environment:
      - DATABASE_URL=postgresql+asyncpg://app_user:${POSTGRES_PASSWORD}@postgres:5432/ai_workflow_db
      - QDRANT_URL=https://qdrant:6333
      - OLLAMA_URL=http://ollama:11434
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    depends_on:
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      ollama:
        condition: service_healthy
    ports:
      - "8002:8002"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8002/health --max-time 3 || exit 1"]
      interval: 20s
      timeout: 5s
      retries: 3
      start_period: 45s

  # Learning Service - Cognitive learning and pattern recognition
  learning-service:
    build:
      context: ./app/learning_service
      dockerfile: Dockerfile
    image: ai_workflow_engine/learning-service
    <<: *base-service-properties
    command: ["python", "-m", "uvicorn", "main_minimal:app", "--host", "0.0.0.0", "--port", "8005"]
    environment:
      - DATABASE_URL=postgresql://app_user:${POSTGRES_PASSWORD}@postgres:5432/ai_workflow_db
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=2
      - REDIS_USER=lwe-app
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - QDRANT_URL=https://qdrant:6333
      - OLLAMA_URL=http://ollama:11434
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      ollama:
        condition: service_healthy
    ports:
      - "8003:8005"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8005/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Perception Service - AI perception and data processing
  perception-service:
    build:
      context: ./app/perception_service
      dockerfile: Dockerfile
    image: ai_workflow_engine/perception-service
    <<: *base-service-properties
    environment:
      - OLLAMA_URL=http://ollama:11434
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    depends_on:
      ollama:
        condition: service_healthy
    ports:
      - "8004:8004"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8004/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Reasoning Service - Logical reasoning and decision analysis
  reasoning-service:
    build:
      context: ./app/reasoning_service
      dockerfile: Dockerfile
      target: production
    image: ai_workflow_engine/reasoning-service
    <<: *base-service-properties
    environment:
      - DATABASE_URL=postgresql://app_user:${POSTGRES_PASSWORD}@postgres:5432/ai_workflow_db
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=3
      - REDIS_USER=lwe-app
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - OLLAMA_URL=http://ollama:11434
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
    ports:
      - "8005:8005"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8005/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Infrastructure Recovery Service - Predictive monitoring and automated recovery
  infrastructure-recovery-service:
    build:
      context: ./app/infrastructure_recovery_service
      dockerfile: Dockerfile
    image: ai_workflow_engine/infrastructure-recovery-service
    <<: *base-service-properties
    environment:
      - DATABASE_URL=postgresql://app_user:${POSTGRES_PASSWORD}@postgres:5432/ai_workflow_db
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=4
      - REDIS_USER=lwe-app
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - PROMETHEUS_URL=http://prometheus:9090
      - ALERTMANAGER_URL=http://alertmanager:9093
      - AUTO_SCALING_ENABLED=true
      - ROLLBACK_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      prometheus:
        condition: service_healthy
    ports:
      - "8010:8010"
    volumes:
      # Mount Docker socket for container management
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # Mount certificate volume for SSL support
      - type: volume
        source: certs
        target: /tmp/certs-volume
        read_only: true
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8010/health --max-time 5 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    # Privileged mode for Docker operations (use with caution in production)
    privileged: true

  postgres:
    build:
      context: .
      dockerfile: docker/postgres/Dockerfile
    <<: *base-service-properties
    security_opt:
      - apparmor:unconfined
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - type: volume
        source: certs
        target: /tmp/certs-volume
        read_only: true
      - ./docker/postgres/01-init-db.sh:/docker-entrypoint-initdb.d/01-init-db.sh:ro
      - ./docker/postgres/docker-entrypoint-wrapper.sh:/usr/local/bin/docker-entrypoint-wrapper.sh:ro
    secrets:
      - POSTGRES_PASSWORD
    environment:
      - POSTGRES_USER=app_user
      - POSTGRES_DB=ai_workflow_db
      - POSTGRES_PASSWORD_FILE=/run/secrets/POSTGRES_PASSWORD
      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256
    entrypoint: /usr/local/bin/docker-entrypoint-wrapper.sh
    # Explicitly define the command to pass to the entrypoint wrapper. This
    # command includes the necessary flags to enable SSL and point PostgreSQL
    # to the isolated certificate location, which is populated by the wrapper.
    command:
      - "postgres"
      - "-c"
      - "ssl=on"
      - "-c"
      - "ssl_cert_file=/etc/certs/postgres/server.crt"
      - "-c"
      - "ssl_key_file=/etc/certs/postgres/server.key"
      - "-c"
      - "ssl_ca_file=/etc/certs/postgres/root.crt"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U app_user -d ai_workflow_engine -h localhost"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  pgbouncer:
    build:
      context: .
      dockerfile: docker/pgbouncer/Dockerfile
    image: ai_workflow_engine/pgbouncer
    <<: *base-service-properties
    volumes:
      # Mount the shared certs volume to a temporary location for the wrapper to use.
      - type: volume
        source: certs
        target: /tmp/certs-volume
        read_only: true
      - ./config/pgbouncer/pgbouncer.ini:/etc/pgbouncer/pgbouncer.ini:ro
      - ./docker/pgbouncer/entrypoint-wrapper.sh:/usr/local/bin/entrypoint-wrapper.sh:ro
    secrets:
      - pgbouncer_users
      - POSTGRES_PASSWORD
    security_opt:
      - apparmor:unconfined
    # The entrypoint is now a wrapper script that generates the auth file.
    entrypoint: /usr/local/bin/entrypoint-wrapper.sh
    # The wrapper will copy certs from /tmp/certs-volume/pgbouncer to /etc/pgbouncer/certs.
    # The pgbouncer.ini file must be updated to look for certs in this new private path.
    # The original command is passed to the wrapper script to be executed after setup.
    # The entrypoint-wrapper.sh script handles all setup, including certificate
    # isolation, auth file generation, and config file processing. It then
    # executes the command passed here. This command starts the pgbouncer process
    # as the 'pgbouncer' user, using the config file generated by the wrapper.
    command: ["su-exec", "pgbouncer", "pgbouncer", "/tmp/pgbouncer.processed.ini"]
    environment:
      # This is required by pgbouncer.ini for the connection string to the database.
      - POSTGRES_HOST=postgres
      # --- Entrypoint Wrapper Configuration ---
      # The wrapper script uses these variables to generate the auth file.
      - PGBOUNCER_AUTH_FILE=/etc/pgbouncer/generated/pgbouncer_users_processed
      - PGBOUNCER_USER_LIST_FILE=/run/secrets/pgbouncer_users
      - PGBOUNCER_TARGET_USER=app_user
      - POSTGRES_SUPERUSER=app_user
      - POSTGRES_DB=ai_workflow_db
      - POSTGRES_PASSWORD_FILE=/run/secrets/POSTGRES_PASSWORD
    healthcheck:
      # The admin user for pgbouncer is 'app_user', not 'pgbouncer'.
      # The admin user for pgbouncer is 'pgbouncer', as defined by the auth_query pattern.
      # Its password is the same as the main postgres password for simplicity. The healthcheck uses
      # sslmode=require (SSL encrypted but no hostname verification) to avoid certificate hostname
      # validation issues while maintaining encrypted connections for reliability.
      test: ["CMD-SHELL", "PGPASSWORD=$(cat /run/secrets/POSTGRES_PASSWORD) psql 'host=localhost port=6432 dbname=pgbouncer user=app_user sslmode=require' -c 'SHOW VERSION;' --quiet -w || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 10s

  redis:
    image: redis:7-alpine
    <<: *base-service-properties
    volumes:
      - redis_data:/data
      - ./config/redis/redis.conf:/usr/local/etc/redis/redis.conf:ro
    secrets:
      - redis_users_acl
      - REDIS_PASSWORD
    command: redis-server /usr/local/etc/redis/redis.conf
    healthcheck:
      # The healthcheck now uses the new user and password to authenticate.
      test: ["CMD-SHELL", "redis-cli -u redis://lwe-app:$$(cat /run/secrets/REDIS_PASSWORD)@localhost:6379 ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  qdrant:
    # Build a custom image to include 'curl' for the healthcheck, as the official
    # qdrant/qdrant image is distroless and lacks shell utilities.
    build: { context: ., dockerfile: docker/qdrant/Dockerfile }
    image: ai_workflow_engine/qdrant
    <<: *base-service-properties
    volumes:
      # Mount the shared certs volume to a temporary location for the wrapper to use.
      - qdrant_data:/qdrant/storage
      - type: volume
        source: certs
        target: /tmp/certs-volume
        read_only: true
      - ./config/qdrant/config.yaml:/qdrant/config/production.yaml:ro
      # This assumes a new wrapper script exists at docker/qdrant/entrypoint-wrapper.sh
      - ./docker/qdrant/entrypoint-wrapper.sh:/usr/local/bin/entrypoint-wrapper.sh:ro
      - ./docker/qdrant/run.sh:/usr/local/bin/run.sh:ro
    secrets:
      - QDRANT_API_KEY
    # The entrypoint is the generic wrapper to handle certificate isolation.
    entrypoint: /usr/local/bin/entrypoint-wrapper.sh
    command: ["sh", "-c", "exec /usr/local/bin/run.sh"]
    environment:
      - QDRANT__SERVICE__API_KEY_FILE=/run/secrets/QDRANT_API_KEY
      - SERVICE_NAME=qdrant # For the entrypoint wrapper
      - QDRANT__SERVICE__ENABLE_TLS=true
      # Paths are corrected to the private location created by the wrapper.
      - QDRANT__TLS__CERT=/etc/certs/qdrant/unified-cert.pem
      - QDRANT__TLS__KEY=/etc/certs/qdrant/unified-key.pem
      # Provide the CA cert to ensure the server can present a full trust chain.
      - QDRANT__TLS__CA_CERT=/etc/certs/qdrant/rootCA.pem
    healthcheck:
      # The qdrant base image has curl, not wget. Using curl to fix the healthcheck.
      test: ["CMD", "curl", "-f", "-k", "https://localhost:6333/healthz"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          # Prevent OOM kills by capping memory usage.
          memory: 2G
        reservations:
          # Request a minimum amount of memory on startup.
          memory: 1G

  prometheus:
    build:
      context: .
      dockerfile: docker/prometheus/Dockerfile
    image: ai_workflow_engine/prometheus
    # The user: "nobody" directive is removed. The official image's entrypoint
    # runs as root to fix volume permissions, then drops to the 'nobody' user.
    # The custom image seems to have a `USER nobody` instruction, which breaks the
    # entrypoint wrapper's permission-sensitive setup. We override it to start as root.
    user: root
    <<: *base-service-properties
    ports:
      - "9090:9090"  # Prometheus web UI and API
    volumes:
      - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
      # Mount the shared certs volume to a temporary location for the wrapper to use.
      - type: volume
        source: certs
        target: /tmp/certs-volume
        read_only: true
      # This assumes a new wrapper script exists at docker/prometheus/entrypoint-wrapper.sh
      - ./docker/prometheus/entrypoint-wrapper.sh:/usr/local/bin/entrypoint-wrapper.sh:ro
    # The entrypoint is the generic wrapper to handle certificate isolation.
    entrypoint: /usr/local/bin/entrypoint-wrapper.sh
    command:
      # The executable must be the first argument passed to the wrapper script.
      - '/bin/prometheus'
      - '--storage.tsdb.path=/prometheus'
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--web.listen-address=0.0.0.0:9090'
      - '--web.enable-admin-api'
      - '--web.enable-lifecycle'
    secrets:
      - QDRANT_API_KEY
    environment:
      - SERVICE_NAME=prometheus
      # The wrapper will run the command as the 'nobody' user after setup.
      - RUN_AS_USER=nobody:nobody
    # The prometheus.yml config must be updated to look for CA certs in the new
    # private path: /etc/prometheus/certs/rootCA.pem
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/-/healthy", "--max-time", "2"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s

  grafana:
    image: grafana/grafana:latest
    <<: *base-service-properties
    ports:
      - "3000:3000"  # Grafana web UI
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./config/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health --max-time 3 || exit 1"]
      interval: 20s
      timeout: 5s
      retries: 3
      start_period: 30s
    depends_on:
      prometheus: { condition: service_healthy }

  alertmanager:
    image: prom/alertmanager:latest
    <<: *base-service-properties
    ports:
      - "9093:9093"  # AlertManager web UI
    volumes:
      - alertmanager_data:/alertmanager
      - ./config/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - type: volume
        source: certs
        target: /tmp/certs-volume
        read_only: true
      - ./docker/alertmanager/entrypoint-wrapper.sh:/usr/local/bin/entrypoint-wrapper.sh:ro
    entrypoint: ["/bin/sh", "/usr/local/bin/entrypoint-wrapper.sh"]
    command:
      - '/bin/alertmanager'
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
      - '--web.listen-address=0.0.0.0:9093'
      - '--cluster.listen-address='
    secrets:
      - API_KEY
    environment:
      - SERVICE_NAME=alertmanager
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    depends_on:
      prometheus: { condition: service_healthy }

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    <<: *base-service-properties
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    privileged: true

  redis_exporter:
    build:
      context: .
      dockerfile: docker/redis-exporter/Dockerfile
    image: ai_workflow_engine/redis_exporter
    <<: *base-service-properties
    entrypoint: /usr/local/bin/entrypoint-wrapper.sh
    command: ["sh", "-c", "exec /usr/local/bin/run.sh"]
    secrets:
      - REDIS_PASSWORD
    environment:
      - SERVICE_NAME=redis_exporter
    volumes:
      - type: volume
        source: certs
        target: /tmp/certs-volume
        read_only: true
      - ./docker/redis-exporter/entrypoint-wrapper.sh:/usr/local/bin/entrypoint-wrapper.sh:ro
      - ./docker/redis-exporter/run.sh:/usr/local/bin/run.sh:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9121/metrics"]
      interval: 30s
      timeout: 5s
      retries: 2
    depends_on:
      redis:
        condition: service_healthy

  postgres_exporter:
    build:
      context: .
      dockerfile: docker/postgres-exporter/Dockerfile
    image: ai_workflow_engine/postgres_exporter
    <<: *base-service-properties
    user: root
    entrypoint: /usr/local/bin/entrypoint-wrapper.sh
    command:
      - "sh"
      - "-c"
      # The wrapper executes this command. It exports the password for libpq to use.
      - "export PGPASSWORD=$(cat /run/secrets/POSTGRES_PASSWORD) && exec postgres_exporter"
    secrets:
      - POSTGRES_PASSWORD
    environment:
      # The DATA_SOURCE_NAME is the primary method for configuring the exporter.
      # The password is supplied separately via the PGPASSWORD environment variable,
      # which is set by the 'command' directive for better security.
      - DATA_SOURCE_NAME=postgresql://app_user@postgres:5432/ai_workflow_engine?sslmode=require
      - SERVICE_NAME=postgres_exporter
    volumes:
      - type: volume
        source: certs
        target: /tmp/certs-volume
        read_only: true
      # The entrypoint is the generic wrapper to handle certificate isolation.
      - ./docker/postgres-exporter/entrypoint-wrapper.sh:/usr/local/bin/entrypoint-wrapper.sh:ro
      - ./config/postgres_exporter.yml:/postgres_exporter.yml:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9187/metrics"]
      interval: 30s
      timeout: 10s
      retries: 2
    depends_on:
      postgres:
        condition: service_healthy

  pgbouncer_exporter:
    build:
      context: .
      dockerfile: docker/pgbouncer-exporter/Dockerfile
    image: ai_workflow_engine/pgbouncer_exporter
    <<: *base-service-properties
    entrypoint: /usr/local/bin/entrypoint-wrapper.sh
    command: ["sh", "-c", "exec /usr/local/bin/run.sh"]
    secrets:
      - POSTGRES_PASSWORD
    environment:
      - SERVICE_NAME=pgbouncer_exporter
    volumes:
      - type: volume
        source: certs
        target: /tmp/certs-volume
        read_only: true
      - ./docker/pgbouncer-exporter/entrypoint-wrapper.sh:/usr/local/bin/entrypoint-wrapper.sh:ro
      - ./docker/pgbouncer-exporter/run.sh:/usr/local/bin/run.sh:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9127/metrics"]
      interval: 30s
      timeout: 5s
      retries: 2
      start_period: 10s
    depends_on:
      pgbouncer:
        condition: service_healthy

  ollama:
    # Build a custom image to include 'curl' for the healthcheck, as the official
    # ollama/ollama image is minimal and lacks shell utilities.
    build:
      context: .
      dockerfile: docker/ollama/Dockerfile
    image: ai_workflow_engine/ollama
    <<: [*base-service-properties, *gpu-deploy]
    ports:
      - "11434:11434"
    volumes:
      # This is important! It saves your models outside the container.
      - ollama_data:/root/.ollama
    environment:
      # Listen on all interfaces to be accessible from other containers.
      - OLLAMA_HOST=0.0.0.0
    # The healthcheck is updated to align with orchestration.md, using a direct
    # API call which is more robust than using the CLI. The start_period is
    # also reduced as the server starts quickly.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 60s

  ollama-pull-llama:
    image: ollama/ollama
    # This service is refactored to use the external `pull_models_if_needed.sh`
    # script. This is cleaner, idempotent, and consistent with docker-stack.yml,
    # avoiding the complexity and potential brittleness of a large inline script.
    volumes:
      - ollama_data:/root/.ollama
      - ./scripts/pull_models_if_needed.sh:/usr/local/bin/pull_models_if_needed.sh:ro
    entrypoint: /bin/sh
    # The command now simply executes the mounted script. The script itself handles
    # checking for and pulling the necessary models.
    command: /usr/local/bin/pull_models_if_needed.sh
    # This network attachment is crucial for resolving the 'ollama' hostname.
    networks: [ai_workflow_engine_net]
    environment:
      # Point the ollama CLI to the HTTP service hostname
      - OLLAMA_HOST=http://ollama:11434
    restart: 'no'
    depends_on:
      ollama:
        condition: service_healthy

  api-migrate:
    build: { context: ., dockerfile: docker/api/Dockerfile }
    image: ai_workflow_engine/api
    # The entrypoint is the generic wrapper to handle certificate isolation.
    entrypoint: /usr/local/bin/entrypoint-wrapper.sh
    command: ["sh", "-c", "exec /usr/local/bin/run-migrate.sh"]
    restart: 'no'
    networks: [ai_workflow_engine_net]
    volumes:
      # This assumes a new wrapper script exists at docker/api/entrypoint-wrapper.sh
      - type: volume
        source: certs
        target: /tmp/certs-volume
        read_only: true
      - ./docker/api/entrypoint-wrapper.sh:/usr/local/bin/entrypoint-wrapper.sh:ro
      - ./docker/api/run-migrate-fixed.sh:/usr/local/bin/run-migrate.sh:ro
    environment:
      # Tell the wrapper to use the 'api' service's certificates.
      - SERVICE_NAME=api
      - PYTHONPATH=/app
      # Provide the host for Pydantic settings validation, even though DATABASE_URL is used for the connection.
      - POSTGRES_HOST=postgres
      - POSTGRES_USER=app_user
      - POSTGRES_PORT=5432
      - POSTGRES_DB=ai_workflow_db
    working_dir: /app
    # The worker service uses an env_file, and migrations likely need it too for consistent settings loading.
    env_file: [".env"]
    secrets: [POSTGRES_PASSWORD, JWT_SECRET_KEY, API_KEY, QDRANT_API_KEY, postgres_user, postgres_db]
    # Migrations connect directly to postgres, so we depend on it directly.
    depends_on:
      postgres: { condition: service_healthy }

  api:
    # Re-use the api-migrate build context
    image: ai_workflow_engine/api # This will be the same image as api-migrate
    <<: *base-service-properties
    security_opt:
      - apparmor:unconfined
    entrypoint: /usr/local/bin/entrypoint-wrapper.sh
    command: ["sh", "-c", "exec /usr/local/bin/run.sh"]
    volumes:
      - type: volume
        source: certs
        target: /tmp/certs-volume
        read_only: true
      - ./docker/api/entrypoint-wrapper.sh:/usr/local/bin/entrypoint-wrapper.sh:ro
      - ./docker/api/run.sh:/usr/local/bin/run.sh:ro
      - .:/project:rw
    secrets: [POSTGRES_PASSWORD, REDIS_PASSWORD, JWT_SECRET_KEY, CSRF_SECRET_KEY, QDRANT_API_KEY, API_KEY, admin_email, admin_password, google_client_id, google_client_secret]
    env_file:
      - .env
      - local.env
    environment:
      - GOOGLE_CLIENT_ID_FILE=/run/secrets/google_client_id
      - GOOGLE_CLIENT_SECRET_FILE=/run/secrets/google_client_secret
      - POSTGRES_HOST=postgres
      - POSTGRES_USER=app_user
      - POSTGRES_PORT=5432
      # DATABASE_URL removed - will be computed from components in config.py with proper SSL
      - PYTHONPATH=/project
      # Explicitly define the Ollama URL for consistency with the worker service.
      - OLLAMA_API_BASE_URL=http://ollama:11434
      - POSTGRES_DB=ai_workflow_db
      # Set the default model to one that is confirmed to be available.
      - OLLAMA_GENERATION_MODEL_NAME=llama3.2:3b
      - SERVICE_NAME=api # For the entrypoint wrapper
    working_dir: /project/app
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health --max-time 3 || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s
    depends_on:
      postgres: { condition: service_healthy }
      pgbouncer: { condition: service_healthy }
      redis: { condition: service_healthy }
      qdrant: { condition: service_healthy }
      # The API service fetches the model list from Ollama, so it must wait for
      # Ollama to be healthy to prevent race conditions on startup.
      ollama: { condition: service_healthy }

  api-create-admin:
    build: { context: ., dockerfile: docker/api/Dockerfile }
    image: ai_workflow_engine/api
    <<: *base-service-properties
    entrypoint: /usr/local/bin/entrypoint-wrapper.sh
    command: ["sh", "-c", "exec /app/scripts/create_admin.sh"]
    restart: 'no'
    networks: [ai_workflow_engine_net]
    volumes:
      - type: volume
        source: certs
        target: /tmp/certs-volume
        read_only: true
      - ./docker/api/entrypoint-wrapper.sh:/usr/local/bin/entrypoint-wrapper.sh:ro
      - ./scripts/create_admin.sh:/app/scripts/create_admin.sh:ro
    environment:
      - SERVICE_NAME=api
      - PYTHONPATH=/app
      - POSTGRES_HOST=postgres
      - POSTGRES_USER=app_user
      - POSTGRES_PORT=5432
      - POSTGRES_DB=ai_workflow_db
    working_dir: /app
    secrets: [POSTGRES_PASSWORD, JWT_SECRET_KEY, API_KEY, QDRANT_API_KEY, admin_email, admin_password]
    depends_on:
      api-migrate: { condition: service_completed_successfully }


  worker:
    # Re-use the worker build context
    build:
      context: .
      dockerfile: docker/worker/Dockerfile
    image: ai_workflow_engine/worker # This will be the same image as worker
    security_opt:
      - apparmor:unconfined
    entrypoint: /usr/local/bin/entrypoint-wrapper.sh
    command: ["sh", "-c", "exec /usr/local/bin/run.sh"]
    <<: [*base-service-properties, *gpu-deploy]
    volumes:
      # This assumes a new wrapper script exists at docker/worker/entrypoint-wrapper.sh
      - type: volume
        source: certs
        target: /tmp/certs-volume
        read_only: true
      - ./docker/worker/entrypoint-wrapper.sh:/usr/local/bin/entrypoint-wrapper.sh:ro
      - ./docker/worker/run.sh:/usr/local/bin/run.sh:ro
      - ./docker/worker/healthcheck.sh:/usr/local/bin/healthcheck.sh:ro
      - ./app:/app:rw
    secrets: [POSTGRES_PASSWORD, JWT_SECRET_KEY, QDRANT_API_KEY, API_KEY, REDIS_PASSWORD, google_client_id, google_client_secret]
    env_file:
      - .env
    environment:
      - SERVICE_NAME=worker # For the entrypoint wrapper
      - PYTHONPATH=/
      # Point the worker to the HTTP Ollama endpoint
      - OLLAMA_API_BASE_URL=http://ollama:11434
      # Provide host for Pydantic settings validation, even though DATABASE_URL is now used for the connection.
      - POSTGRES_HOST=pgbouncer
      - POSTGRES_USER=app_user
      - POSTGRES_PORT=6432
      - POSTGRES_DB=ai_workflow_db
      # Set the default model to one that is confirmed to be available.
      - OLLAMA_GENERATION_MODEL_NAME=llama3.2:3b
    working_dir: /app
    healthcheck:
      test: ["CMD-SHELL", "/usr/local/bin/healthcheck.sh"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 120s
    depends_on:
      pgbouncer: { condition: service_healthy }
      redis: { condition: service_healthy }
      ollama-pull-llama: { condition: service_completed_successfully }
      ollama: { condition: service_healthy }
      qdrant: { condition: service_healthy }


  webui:
    # SvelteKit WebUI with cosmic hero landing page
    build:
      # Set the build context to the SvelteKit webui-next directory
      context: ./app/webui-next
      # The Dockerfile path is now relative to the new context
      dockerfile: ../../docker/webui-next/Dockerfile
    image: ai_workflow_engine/webui-next
    <<: *base-service-properties
    # Use Node.js server with correct filename
    command: ["node", "server.js"]
    volumes:
      # Mount certificate volume for SSL support (simplified for now)
      - type: volume
        source: certs
        target: /tmp/certs-volume
        read_only: true
    environment:
      - SERVICE_NAME=webui-next # For the entrypoint wrapper
      - NODE_ENV=production
      - PORT=3001
      - HOSTNAME=0.0.0.0
      # Next.js specific environment variables
      - NEXT_TELEMETRY_DISABLED=1
      # Configure correct API proxy target
      - API_URL=http://api:8000
      - WS_URL=ws://api:8000
    working_dir: /app
    healthcheck:
      test: ["CMD-SHELL", "wget --quiet --tries=1 --spider --timeout=3 http://0.0.0.0:3001/ || exit 1"]
      interval: 25s
      timeout: 5s
      retries: 3
      start_period: 45s
    depends_on:
      api: { condition: service_healthy }

  caddy_reverse_proxy:
    image: caddy:2-alpine
    <<: *base-service-properties
    # The entrypoint is the generic wrapper to handle certificate isolation.
    entrypoint: /usr/local/bin/entrypoint-wrapper.sh
    command: ["caddy", "run", "--config", "/etc/caddy/Caddyfile", "--adapter", "caddyfile"]
    ports:
      - "80:80"
      - "443:443"
      - "8443:8443"
    volumes:
      - ./config/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      # Temporarily disable custom certificates to allow Let's Encrypt automatic HTTPS
      # - type: volume
      #   source: certs
      #   target: /tmp/certs-volume
      #   read_only: true
      - caddy_data:/data  # Required for Let's Encrypt certificate storage
      - caddy_config:/config  # Required for Caddy configuration persistence
      - ./docker/caddy_reverse_proxy/entrypoint-wrapper.sh:/usr/local/bin/entrypoint-wrapper.sh:ro
    environment:
      - SERVICE_NAME=caddy_reverse_proxy # For the entrypoint wrapper
      - DOMAIN=${DOMAIN:-aiwfe.com}
      - DNS_PROVIDER=${DNS_PROVIDER:-cloudflare}
      - DNS_API_TOKEN=${DNS_API_TOKEN}
      - ACME_EMAIL=${ACME_EMAIL:-markuszvirbulis@gmail.com}
    # The Caddyfile must be updated to look for certs in the new private path.
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://127.0.0.1:2019/config/"]
      interval: 15s
      timeout: 3s
      retries: 3
    depends_on:
      api: { condition: service_healthy }
      webui: { condition: service_started }

  # Node Exporter for system-level metrics
  node_exporter:
    image: prom/node-exporter:latest
    <<: *base-service-properties
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    # Port binding removed to prevent dual access path conflicts
    # Prometheus will access via container networking: node_exporter:9100
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9100/metrics"]
      interval: 30s
      timeout: 30s
      retries: 2

  # Blackbox Exporter for synthetic monitoring
  blackbox_exporter:
    image: prom/blackbox-exporter:latest
    <<: *base-service-properties
    volumes:
      - ./config/blackbox/blackbox.yml:/config/blackbox.yml:ro
    command:
      - '--config.file=/config/blackbox.yml'
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9115/metrics"]
      interval: 30s
      timeout: 10s
      retries: 2

  # Jaeger for distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:latest
    <<: *base-service-properties
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - COLLECTOR_ZIPKIN_HOST_PORT=:9411
    ports:
      - "16686:16686"  # Jaeger UI
      - "14268:14268"  # Jaeger collector HTTP
      - "6831:6831/udp"  # Jaeger agent UDP
      - "6832:6832/udp"  # Jaeger agent UDP
    volumes:
      - jaeger_data:/badger
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:14269/"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Loki for log aggregation
  loki:
    image: grafana/loki:latest
    <<: *base-service-properties
    volumes:
      - ./config/loki/loki.yml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Promtail for log shipping to Loki
  promtail:
    image: grafana/promtail:latest
    <<: *base-service-properties
    volumes:
      - ./config/promtail/promtail.yml:/etc/promtail/config.yml:ro
      - /var/log:/var/log:ro
      - ./logs:/app/logs:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      loki:
        condition: service_healthy

  # Elasticsearch for advanced log analysis (optional)
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    <<: *base-service-properties
    environment:
      - node.name=elasticsearch
      - cluster.name=ai-workflow-logs
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9200/_cluster/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Kibana for log visualization (optional)
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    <<: *base-service-properties
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - SERVER_NAME=kibana
      - SERVER_HOST=0.0.0.0
    ports:
      - "5601:5601"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5601/api/status"]
      interval: 30s
      timeout: 10s
      retries: 5
    depends_on:
      elasticsearch:
        condition: service_healthy

  # Logstash for log processing (optional)
  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    <<: *base-service-properties
    volumes:
      - ./config/logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro
      - ./config/logstash/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
    environment:
      - "LS_JAVA_OPTS=-Xmx256m -Xms256m"
    depends_on:
      elasticsearch:
        condition: service_healthy

  # Fluent Bit for lightweight log forwarding (alternative to Promtail/Logstash)
  fluent_bit:
    image: fluent/fluent-bit:latest
    <<: *base-service-properties
    volumes:
      - ./config/fluent-bit/fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf:ro
      - ./logs:/app/logs:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    depends_on:
      loki:
        condition: service_healthy

  # This service watches for container failures and triggers the log collection script.
  log-watcher:
    image: docker:27.1-cli # Use the official Docker CLI image
    container_name: ai_workflow_engine-log-watcher-1
    restart: always
    networks:
      - ai_workflow_engine_net
    volumes:
      # Mount the Docker socket to listen to events from the host.
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # Mount your scripts directory into the container.
      - ./scripts:/app/scripts:rw
      # Mount a logs directory to persist the collected logs on the host.
      - ./logs:/app/logs
      # Mount application debug logs for collection
      # Mount application debug logs for collection
      - ./app:/app/application:ro
    # The command to run your watcher script when the container starts.
    # We ensure all scripts are executable within the container to prevent
    # permission errors if they weren't set correctly on the host.
    command: ["sh", "-c", "apk add --no-cache bash jq && bash /app/scripts/_comprehensive_logger.sh"]