{
  "package_id": "gpu_optimization_context_package",
  "package_type": "migrated-context",
  "content": "{\n  \"package_type\": \"GPU_OPTIMIZATION_PRIORITY_1\",\n  \"size_limit\": 4000,\n  \"target_agents\": [\n    \"performance-profiler\",\n    \"monitoring-analyst\",\n    \"k8s-architecture-specialist\",\n    \"langgraph-ollama-analyst\"\n  ],\n  \"coordination_metadata\": {\n    \"resource_allocation\": \"PRIMARY_OPTIMIZATION_FOCUS\",\n    \"conflict_prevention\": \"COORDINATE_WITH_SERVICE_OPTIMIZATION\",\n    \"execution_priority\": 1,\n    \"expected_roi\": \"268% tokens/second improvement, 183% VRAM efficiency\"\n  },\n  \"current_status\": {\n    \"gpu_utilization\": \"0-32% across 3x TITAN X GPUs\",\n    \"power_consumption\": \"200W baseline\",\n    \"performance_baseline\": \"Current throughput measurements needed\",\n    \"target_utilization\": \"55-75% optimal range\",\n    \"expected_power\": \"300-400W optimized consumption\"\n  },\n  \"ollama_configuration\": {\n    \"current_config\": \"/home/marku/ai_workflow_engine/config/ollama/ollama-optimized.env\",\n    \"key_parameters\": {\n      \"OLLAMA_NUM_PARALLEL\": \"2 (current)\",\n      \"OLLAMA_MAX_LOADED_MODELS\": \"3\",\n      \"OLLAMA_FLASH_ATTENTION\": \"true\",\n      \"OLLAMA_GPU_OVERHEAD\": \"0\",\n      \"OLLAMA_SCHED_SPREAD\": \"true\",\n      \"OLLAMA_MAX_QUEUE\": \"256\"\n    },\n    \"optimization_targets\": {\n      \"multi_model_distribution\": \"Distribute models across 3 GPUs\",\n      \"memory_optimization\": \"12GB VRAM per GPU utilization\",\n      \"parallel_processing\": \"Increase OLLAMA_NUM_PARALLEL to 4-6\",\n      \"queue_management\": \"Optimize request distribution\"\n    }\n  },\n  \"implementation_strategy\": {\n    \"approach\": \"CONFIGURATION_OPTIMIZATION\",\n    \"risk_level\": \"MEDIUM (22%)\",\n    \"success_probability\": \"78%\",\n    \"timeline\": \"45-60 minutes\",\n    \"rollback_plan\": \"Revert to current ollama-optimized.env\"\n  },\n  \"monitoring_requirements\": {\n    \"gpu_metrics\": [\n      \"GPU utilization per device\",\n      \"VRAM usage per model\",\n      \"Temperature monitoring\",\n      \"Power consumption tracking\"\n    ],\n    \"performance_metrics\": [\n      \"Tokens per second throughput\",\n      \"Model loading times\",\n      \"Queue processing speed\",\n      \"Response latency\"\n    ],\n    \"validation_criteria\": [\n      \"Sustained 55-75% GPU utilization\",\n      \"200-300% throughput improvement\",\n      \"No thermal throttling\",\n      \"Stable multi-model operation\"\n    ]\n  },\n  \"evidence_collection\": {\n    \"baseline_measurements\": \"nvidia-smi, ollama ps outputs\",\n    \"optimization_validation\": \"Before/after performance benchmarks\",\n    \"monitoring_integration\": \"Grafana GPU dashboard updates\",\n    \"success_metrics\": \"Concrete utilization and throughput evidence\"\n  },\n  \"agent_coordination\": {\n    \"performance-profiler\": \"GPU utilization analysis and optimization\",\n    \"monitoring-analyst\": \"Grafana dashboard updates and alerting\",\n    \"k8s-architecture-specialist\": \"Container resource allocation optimization\",\n    \"langgraph-ollama-analyst\": \"Ollama configuration and model distribution\"\n  },\n  \"historical_patterns\": {\n    \"ml_decision_engine_insights\": \"Configuration approach recommended over hardware changes\",\n    \"similar_optimizations\": \"Container resource allocation patterns show 200-400% improvements\",\n    \"risk_mitigation\": \"Environment variable changes allow rapid rollback\"\n  }\n}",
  "tokens": 314,
  "created_at": "2025-08-18T15:14:25.777778",
  "last_accessed": "2025-08-18T15:14:25.777792",
  "priority": "medium",
  "context_tags": [
    "migrated",
    "context-package"
  ]
}